{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyN/ya6CwZONHuqo/NjrAe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jac0bLii/Chenxi-Li-1007405880--STA365HW/blob/main/HW7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEuZY4wQDpHp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chenxi Li\n",
        "1007405880"
      ],
      "metadata": {
        "id": "S_M1iq5CEh5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Describe how the posterior predictive distribution is created for mixture models\n",
        "\n",
        "In general, if we want to creat a posterior predictive distribbution for mixture models, we need to follow these steps\n",
        "\n",
        "\n",
        "Step 1: we need to definne the mixture model's strucutre, i.e. how the data are distributed, i.e. a normal and a binomial...\n",
        "\n",
        "Step 2: we need to use techniques such as Markov Chain Monte Carlo (MCMC) estimate the parameter of our mixture model.\n",
        "\n",
        "Step 3: We need follos the Bayes rule's do the posterior predictive distribution. as detailed shows in Question 2.\n",
        "\n",
        "\n",
        "Step 4: Fianlly, we need to do the model validation, check the quality of our estimated posterior and samppling performs using techniques like cross validation method.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRMUgoeKDr74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Describe how the posterior predictive distribution is created in general\n",
        "\n",
        "In gerneral, if we want to creat a posterior predictive distribbution, we need to follow these steps\n",
        "\n",
        "We have the bayes rule in general:\n",
        "\n",
        "P(θ|data) = $\\frac{P(data|θ)\\cdot P(θ)}{P(data)}$\n",
        "\n",
        "we need using existed data (data) to predict the unknown (θ).\n",
        "\n",
        "Step 1: We need to find the prior distribution ,P(θ), based on any prior knowledge, beliefs, or assumptions about the parameters we interested in.\n",
        "\n",
        "Step 2: Then, we get our likelihood,P(data|θ), based on our marginal likelihood and assumed prior as it is derived based on the assumed data-generating process and represents the probability distribution of the observed data conditioned on the parameter values.\n",
        "\n",
        "Step 3: Then, we estimaed our posterior P(θ|data) based on likelihood and prior.\n",
        "\n",
        "Step 4: Once the posterior distribution, P(θ|data), is obtained, the posterior predictive distribution is created by integrating over this distribution to predict the outcomes for new, unseen data points.\n",
        "\n",
        "Step 5: We have: P(new data∣observed data)=∫P(new data∣θ)×P(θ∣observed data)dθ, and we have both P(new data∣θ) and P(θ∣observed data). Then we can do the computation.\n",
        "\n",
        "Step 6: We use the statistical techniques such as Markov Chain Monte Carlo and MCMC helps on computation.\n",
        "\n",
        "Then we finish our general steps of posterior predictive distribution."
      ],
      "metadata": {
        "id": "JE7jcp8nDzGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$.\n",
        "\n",
        "\n",
        "From reading this, I learned that there are three types of missing data:\n",
        "\n",
        "Missing Completely at Random (MCAR)\n",
        "\n",
        "Missing at Random (MAR)\n",
        "\n",
        "Missing Not at Random (MNAR)\n",
        "\n",
        "Case 1 Missing Completely at Random (MCAR): This is not important to our estimation accuracy. As it is MCAR, we have this distribution of the missing data are in a manner that is unrelated to both the observed and unobserved parts of the realised data.\n",
        "\n",
        "Case 2 Missing at Random (MAR): This is somehow  ignorable because the missingness can be function of the observed data, so if we loss these data, our prediction percision will decrease, but not influence the general pattern. Also, we can also estimate these missing data by computing the distribution of existed data and do the estimation.\n",
        "\n",
        "Case 3 Missing Not at Random (MNAR): This is the most important, Also, the imputation and estimation more generally may become more difficulty in this final case because of the risk of confounding. As it MNAR, we have the loss data are not compelety a function of the observaed data. Thus, we need a complex estimation: We do Full Information Maximum Likelihood to estimate the full data distribbution, then we Sampling from the Implied Distribution and do the correlation test between imputed metrics data. After that, we do the bootstramping to discuss the sensitivity.\n",
        "\n"
      ],
      "metadata": {
        "id": "jIZZdofZDzgX"
      }
    }
  ]
}